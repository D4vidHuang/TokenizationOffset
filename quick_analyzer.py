#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿç‰ˆå¤šè¯­è¨€åˆ†æå™¨ - ä½¿ç”¨å·²æœ‰çš„ç¼–è¯‘åº“
"""

import os
import json
import argparse
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional

from tree_sitter import Language, Parser
from transformers import AutoTokenizer
import warnings
warnings.filterwarnings('ignore')

class QuickMultiLanguageAnalyzer:
    """å¿«é€Ÿç‰ˆå¤šè¯­è¨€åˆ†æå™¨ - ä½¿ç”¨å·²ç¼–è¯‘çš„åº“"""
    
    def __init__(self, model_name: str = "gpt2"):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # è¯­è¨€é…ç½®
        self.language_configs = {
            'python': {'symbol': 'python', 'extensions': ['.py']},
            'javascript': {'symbol': 'javascript', 'extensions': ['.js']},
            'typescript': {'symbol': 'typescript', 'extensions': ['.ts']},
            'java': {'symbol': 'java', 'extensions': ['.java']},
            'c': {'symbol': 'c', 'extensions': ['.c', '.h']},
            'cpp': {'symbol': 'cpp', 'extensions': ['.cpp', '.cc', '.cxx', '.hpp']},
            'csharp': {'symbol': 'c_sharp', 'extensions': ['.cs']},
            'go': {'symbol': 'go', 'extensions': ['.go']},
            'ruby': {'symbol': 'ruby', 'extensions': ['.rb']},
            'rust': {'symbol': 'rust', 'extensions': ['.rs']},
            'scala': {'symbol': 'scala', 'extensions': ['.scala']}
        }
        
        self.parsers = {}
        self.languages = {}
        self._setup_parsers()
    
    def _setup_parsers(self):
        """è®¾ç½®è§£æå™¨ - ä½¿ç”¨å·²æœ‰çš„ç¼–è¯‘åº“"""
        build_dir = Path('./build')
        
        if not build_dir.exists():
            print("é”™è¯¯: build ç›®å½•ä¸å­˜åœ¨")
            return
        
        print("æ­£åœ¨åŠ è½½å·²ç¼–è¯‘çš„è¯­è¨€åº“...")
        
        # å°è¯•åˆå§‹åŒ–æ¯ç§è¯­è¨€
        for lang_name, config in self.language_configs.items():
            try:
                # æŸ¥æ‰¾å¯¹åº”çš„è¯­è¨€åº“æ–‡ä»¶
                possible_paths = [
                    build_dir / f"languages_{lang_name}.so",
                    build_dir / f"languages.so",
                    build_dir / f"multilang_languages.so"
                ]
                
                library_path = None
                for path in possible_paths:
                    if path.exists():
                        library_path = path
                        break
                
                if not library_path:
                    print(f"âœ— {lang_name} è¯­è¨€åº“æ–‡ä»¶ä¸å­˜åœ¨")
                    continue
                
                parser = Parser()
                language = Language(str(library_path), config['symbol'])
                parser.set_language(language)
                
                self.parsers[lang_name] = parser
                self.languages[lang_name] = language
                print(f"âœ“ {lang_name} è§£æå™¨å¯ç”¨ (ä½¿ç”¨ {library_path.name})")
                
            except Exception as e:
                print(f"âœ— {lang_name} è§£æå™¨ä¸å¯ç”¨: {e}")
    
    def get_available_languages(self) -> List[str]:
        """è·å–å¯ç”¨è¯­è¨€åˆ—è¡¨"""
        return list(self.parsers.keys())
    
    def calculate_rule_level_alignment(self, code: str, language: str) -> Tuple[float, Dict]:
        """è®¡ç®—è§„åˆ™çº§åˆ«å¯¹é½åˆ†æ•°"""
        if language not in self.parsers:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€: {language}")
        
        parser = self.parsers[language]
        code_bytes = code.encode('utf-8')
        
        # è§£æä»£ç 
        tree = parser.parse(code_bytes)
        
        # æå–è§„åˆ™
        def extract_rules(node, rules=None):
            if rules is None:
                rules = []
            
            if node.type and not node.type.startswith('ERROR'):
                rules.append({
                    'type': node.type,
                    'start_byte': node.start_byte,
                    'end_byte': node.end_byte,
                    'text': code_bytes[node.start_byte:node.end_byte].decode('utf-8', errors='ignore')[:50]
                })
            
            for child in node.children:
                extract_rules(child, rules)
            
            return rules
        
        rules = extract_rules(tree.root_node)
        
        # Tokenization
        try:
            tokens = self.tokenizer.encode(code)
            token_texts = [self.tokenizer.decode([token]) for token in tokens]
        except Exception as e:
            print(f"Tokenization é”™è¯¯: {e}")
            return 0.0, {}
        
        # è®¡ç®— token è¾¹ç•Œ
        token_boundaries = []
        current_pos = 0
        
        for token_text in token_texts:
            token_start = code.find(token_text, current_pos)
            if token_start != -1:
                token_end = token_start + len(token_text)
                token_boundaries.append((token_start, token_end))
                current_pos = token_end
            else:
                token_boundaries.append((current_pos, current_pos + 1))
                current_pos += 1
        
        # è®¡ç®—å¯¹é½
        aligned_rules = 0
        rule_details = {}
        
        for rule in rules:
            rule_start = rule['start_byte']
            rule_end = rule['end_byte']
            
            start_aligned = any(abs(rule_start - tb[0]) <= 1 for tb in token_boundaries)
            end_aligned = any(abs(rule_end - tb[1]) <= 1 for tb in token_boundaries)
            
            fully_aligned = start_aligned and end_aligned
            if fully_aligned:
                aligned_rules += 1
            
            rule_key = f"{rule['type']}_{rule['start_byte']}_{rule['end_byte']}"
            rule_details[rule_key] = {
                'type': rule['type'],
                'start_aligned': start_aligned,
                'end_aligned': end_aligned,
                'fully_aligned': fully_aligned,
                'text_preview': rule['text'][:50]
            }
        
        alignment_score = (aligned_rules / len(rules) * 100) if rules else 0
        return alignment_score, rule_details
    
    def analyze_language_files(self, code_dir: str, language: str) -> Dict:
        """åˆ†ææŒ‡å®šè¯­è¨€çš„æ‰€æœ‰æ–‡ä»¶"""
        if language not in self.parsers:
            print(f"è·³è¿‡ä¸æ”¯æŒçš„è¯­è¨€: {language}")
            return {}
        
        language_dir = Path(code_dir) / language
        if not language_dir.exists():
            print(f"è¯­è¨€ç›®å½•ä¸å­˜åœ¨: {language_dir}")
            return {}
        
        # æŸ¥æ‰¾æ–‡ä»¶
        extensions = self.language_configs[language]['extensions']
        code_files = []
        for ext in extensions:
            code_files.extend(language_dir.glob(f"*{ext}"))
        
        if not code_files:
            print(f"æ²¡æœ‰æ‰¾åˆ° {language} æ–‡ä»¶")
            return {}
        
        print(f"\nåˆ†æ {language.upper()} ({len(code_files)} ä¸ªæ–‡ä»¶)")
        print("-" * 50)
        
        # åˆ†ææ–‡ä»¶
        file_results = []
        total_rules = 0
        total_aligned = 0
        
        for file_path in code_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    code = f.read()
                
                if not code.strip():
                    continue
                
                score, details = self.calculate_rule_level_alignment(code, language)
                aligned_count = sum(1 for d in details.values() if d['fully_aligned'])
                
                file_results.append({
                    'file': file_path.name,
                    'score': score,
                    'total_rules': len(details),
                    'aligned_rules': aligned_count
                })
                
                total_rules += len(details)
                total_aligned += aligned_count
                
                print(f"  {file_path.name:<20} {score:6.2f}% ({aligned_count}/{len(details)})")
                
            except Exception as e:
                print(f"  {file_path.name:<20} é”™è¯¯: {e}")
        
        if not file_results:
            return {}
        
        # è®¡ç®—ç»Ÿè®¡
        avg_score = sum(r['score'] for r in file_results) / len(file_results)
        overall_alignment = (total_aligned / total_rules * 100) if total_rules > 0 else 0
        
        result = {
            'language': language,
            'file_count': len(file_results),
            'avg_score': avg_score,
            'total_rules': total_rules,
            'total_aligned': total_aligned,
            'overall_alignment': overall_alignment,
            'files': file_results
        }
        
        print(f"\n{language.upper()} æ€»ç»“:")
        print(f"  æ–‡ä»¶æ•°: {result['file_count']}")
        print(f"  å¹³å‡åˆ†æ•°: {result['avg_score']:.2f}%")
        print(f"  æ€»è§„åˆ™æ•°: {result['total_rules']}")
        print(f"  æ€»å¯¹é½æ•°: {result['total_aligned']}")
        print(f"  æ•´ä½“å¯¹é½ç‡: {result['overall_alignment']:.2f}%")
        
        return result
    
    def run_analysis(self, code_dir: str = "code_samples", 
                    target_languages: List[str] = None, 
                    output_dir: str = "results/multilang") -> Dict:
        """è¿è¡Œåˆ†æ"""
        available_languages = self.get_available_languages()
        
        if not available_languages:
            print("é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„è¯­è¨€è§£æå™¨")
            return {}
        
        if target_languages is None:
            target_languages = available_languages
        else:
            target_languages = [lang for lang in target_languages if lang in available_languages]
        
        print("=" * 80)
        print("å¿«é€Ÿå¤šè¯­è¨€ Rule-level Alignment Score åˆ†æ")
        print("=" * 80)
        print(f"å¯ç”¨è¯­è¨€: {' '.join(available_languages)}")
        print(f"åˆ†æè¯­è¨€: {' '.join(target_languages)}")
        
        results = {}
        for language in target_languages:
            result = self.analyze_language_files(code_dir, language)
            if result:
                results[language] = result
        
        # ç”Ÿæˆæ’å
        rankings = []
        if results:
            print(f"\n{'='*60}")
            print("è¯­è¨€æ’å (æŒ‰å¹³å‡åˆ†æ•°)")
            print(f"{'='*60}")
            
            rankings = sorted(results.items(), key=lambda x: x[1]['avg_score'], reverse=True)
            for i, (lang, result) in enumerate(rankings, 1):
                print(f"{i:2d}. {lang:<12} {result['avg_score']:6.2f}% "
                      f"(æ–‡ä»¶: {result['file_count']}, è§„åˆ™: {result['total_rules']})")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        self._save_results(results, rankings, output_dir)
        
        return results
    
    def _save_results(self, results: Dict, rankings: List, output_dir: str):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {
            'model': self.model_name,
            'timestamp': str(Path().resolve()),
            'summary': {
                'total_languages': len(results),
                'total_files': sum(r['file_count'] for r in results.values()),
                'total_rules': sum(r['total_rules'] for r in results.values()),
                'total_aligned': sum(r['total_aligned'] for r in results.values())
            },
            'languages': results,
            'rankings': [
                {
                    'rank': i + 1,
                    'language': lang,
                    'avg_score': result['avg_score'],
                    'file_count': result['file_count'],
                    'total_rules': result['total_rules'],
                    'total_aligned': result['total_aligned']
                }
                for i, (lang, result) in enumerate(rankings)
            ]
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        detailed_file = output_path / f"detailed_analysis_{self.model_name}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç®€åŒ–çš„æ’åæŠ¥å‘Š
        ranking_report = {
            'model': self.model_name,
            'analysis_date': str(Path().resolve()),
            'rankings': detailed_results['rankings'],
            'summary': detailed_results['summary']
        }
        
        ranking_file = output_path / f"language_rankings_{self.model_name}.json"
        with open(ranking_file, 'w', encoding='utf-8') as f:
            json.dump(ranking_report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆè·¨è¯­è¨€æŠ¥å‘Šï¼ˆä¸ºå¯è§†åŒ–å·¥å…·ä½¿ç”¨ï¼‰
        cross_language_report = {
            "model": self.model_name,
            "analysis_date": str(output_path),
            "language_rankings": [
                {
                    "language": lang,
                    "avg_score": result["avg_score"],
                    "total_rules": result["total_rules"],
                    "aligned_rules": result["total_aligned"],
                    "alignment_rate": (result["total_aligned"] / result["total_rules"] * 100) if result["total_rules"] > 0 else 0
                }
                for lang, result in rankings
            ],
            "analysis_summary": {
                "analyzed_languages": len(rankings),
                "total_files": sum(result["file_count"] for _, result in rankings),
                "total_rules": sum(result["total_rules"] for _, result in rankings),
                "total_aligned_rules": sum(result["total_aligned"] for _, result in rankings),
                "average_score": sum(result["avg_score"] for _, result in rankings) / len(rankings) if rankings else 0,
                "overall_alignment_rate": (sum(result["total_aligned"] for _, result in rankings) / sum(result["total_rules"] for _, result in rankings) * 100) if sum(result["total_rules"] for _, result in rankings) > 0 else 0
            }
        }
        
        cross_lang_file = output_path / f"cross_language_report_{self.model_name}.json"
        with open(cross_lang_file, 'w', encoding='utf-8') as f:
            json.dump(cross_language_report, f, ensure_ascii=False, indent=2)
        
        # ä¸ºæ¯ç§è¯­è¨€ä¿å­˜å•ç‹¬çš„è¯¦ç»†æŠ¥å‘Š
        for language, result in results.items():
            lang_dir = output_path / language
            lang_dir.mkdir(exist_ok=True)
            
            lang_file = lang_dir / f"analysis_report_{self.model_name}.json"
            with open(lang_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  - è¯¦ç»†æŠ¥å‘Š: {detailed_file}")
        print(f"  - æ’åæŠ¥å‘Š: {ranking_file}")
        print(f"  - å„è¯­è¨€æŠ¥å‘Š: {output_path}/{{language}}/analysis_report_{self.model_name}.json")

def main():
    parser = argparse.ArgumentParser(description='å¿«é€Ÿå¤šè¯­è¨€åˆ†æå™¨')
    parser.add_argument('--language', help='æŒ‡å®šè¯­è¨€')
    parser.add_argument('--all_languages', action='store_true', help='åˆ†ææ‰€æœ‰è¯­è¨€')
    parser.add_argument('--code_dir', default='code_samples', help='ä»£ç ç›®å½•')
    parser.add_argument('--output_dir', default='results/multilang', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model', default='gpt2', help='Tokenizer æ¨¡å‹')
    
    args = parser.parse_args()
    
    analyzer = QuickMultiLanguageAnalyzer(model_name=args.model)
    
    if args.language:
        target_languages = [args.language]
    elif args.all_languages:
        target_languages = None  # åˆ†ææ‰€æœ‰å¯ç”¨è¯­è¨€
    else:
        target_languages = ['python']  # é»˜è®¤åªåˆ†æ Python
    
    analyzer.run_analysis(args.code_dir, target_languages, args.output_dir)

if __name__ == "__main__":
    main()