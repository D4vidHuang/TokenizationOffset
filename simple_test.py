#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæµ‹è¯•è„šæœ¬ - é…åˆ quick_analyzer.py ä½¿ç”¨
æµ‹è¯•åŸºæœ¬çš„ Tree-sitter è§£æå’Œå¯¹é½åˆ†æ•°è®¡ç®—åŠŸèƒ½
"""

import os
import sys
from pathlib import Path
from tree_sitter import Language, Parser
from transformers import AutoTokenizer

def test_quick_analyzer_functionality():
    """æµ‹è¯• quick_analyzer.py çš„æ ¸å¿ƒåŠŸèƒ½"""
    print("=" * 60)
    print("Quick Analyzer åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥ç¼–è¯‘å¥½çš„è¯­è¨€åº“
        build_dir = Path('./build')
        python_library_path = build_dir / 'languages_python.so'
        
        if not python_library_path.exists():
            print("âŒ æœªæ‰¾åˆ° Python è¯­è¨€åº“")
            print("è¯·å…ˆè¿è¡Œ quick_analyzer.py æ¥ç¼–è¯‘è¯­è¨€åº“")
            return False
        
        print("âœ“ æ‰¾åˆ° Python è¯­è¨€åº“")
        
        # æµ‹è¯• Python è§£æå™¨
        print("\næ­£åœ¨æµ‹è¯• Python è§£æå™¨...")
        parser = Parser()
        
        try:
            python_language = Language(str(python_library_path), 'python')
            parser.set_language(python_language)
            print("âœ“ Python è§£æå™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Python è§£æå™¨åŠ è½½å¤±è´¥: {e}")
            return False
        
        # åˆå§‹åŒ– tokenizer
        print("\næ­£åœ¨åˆå§‹åŒ– tokenizer...")
        try:
            tokenizer = AutoTokenizer.from_pretrained('gpt2')
            print("âœ“ GPT-2 tokenizer åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•ä»£ç æ ·æœ¬
        test_code = '''
def fibonacci(n):
    """è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"""
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# æµ‹è¯•å‡½æ•°
for i in range(10):
    result = fibonacci(i)
    print(f"fibonacci({i}) = {result}")
'''
        
        print("\næ­£åœ¨åˆ†ææµ‹è¯•ä»£ç ...")
        
        # è§£æä»£ç 
        code_bytes = test_code.encode('utf-8')
        tree = parser.parse(code_bytes)
        
        if tree.root_node.has_error:
            print("âŒ ä»£ç è§£æå‡ºç°é”™è¯¯")
            return False
        
        print("âœ“ ä»£ç è§£ææˆåŠŸ")
        
        # æå–è¯­æ³•è§„åˆ™
        def extract_rules(node, rules=None):
            if rules is None:
                rules = []
            
            if node.type and not node.type.startswith('ERROR'):
                rules.append({
                    'type': node.type,
                    'start_byte': node.start_byte,
                    'end_byte': node.end_byte,
                    'start_point': node.start_point,
                    'end_point': node.end_point,
                    'text': code_bytes[node.start_byte:node.end_byte].decode('utf-8', errors='ignore')[:30]
                })
            
            for child in node.children:
                extract_rules(child, rules)
            
            return rules
        
        rules = extract_rules(tree.root_node)
        print(f"âœ“ æå–åˆ° {len(rules)} ä¸ªè¯­æ³•è§„åˆ™")
        
        # Tokenization
        tokens = tokenizer.encode(test_code)
        token_texts = [tokenizer.decode([token]) for token in tokens]
        print(f"âœ“ ç”Ÿæˆ {len(tokens)} ä¸ª tokens")
        
        # è®¡ç®— token è¾¹ç•Œ
        token_boundaries = []
        current_pos = 0
        
        for token_text in token_texts:
            # å¤„ç†ç‰¹æ®Šå­—ç¬¦
            if token_text.strip():
                token_start = test_code.find(token_text, current_pos)
                if token_start != -1:
                    token_end = token_start + len(token_text)
                    token_boundaries.append((token_start, token_end))
                    current_pos = token_end
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨å½“å‰ä½ç½®
                    token_boundaries.append((current_pos, current_pos + 1))
                    current_pos += 1
            else:
                token_boundaries.append((current_pos, current_pos + 1))
                current_pos += 1
        
        print(f"âœ“ è®¡ç®— token è¾¹ç•Œå®Œæˆ")
        
        # è®¡ç®—å¯¹é½åˆ†æ•°
        aligned_rules = 0
        tolerance = 1  # å…è®¸1å­—ç¬¦çš„è¯¯å·®
        
        for rule in rules:
            rule_start = rule['start_byte']
            rule_end = rule['end_byte']
            
            # æ£€æŸ¥èµ·å§‹ä½ç½®å¯¹é½
            start_aligned = any(abs(rule_start - tb[0]) <= tolerance for tb in token_boundaries)
            # æ£€æŸ¥ç»“æŸä½ç½®å¯¹é½
            end_aligned = any(abs(rule_end - tb[1]) <= tolerance for tb in token_boundaries)
            
            if start_aligned and end_aligned:
                aligned_rules += 1
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        alignment_score = (aligned_rules / len(rules) * 100) if rules else 0
        
        print("\n" + "=" * 40)
        print("æµ‹è¯•ç»“æœ")
        print("=" * 40)
        print(f"Rule-level Alignment Score: {alignment_score:.2f}%")
        print(f"æ€»è¯­æ³•è§„åˆ™æ•°: {len(rules)}")
        print(f"å¯¹é½è§„åˆ™æ•°: {aligned_rules}")
        print(f"Token æ€»æ•°: {len(tokens)}")
        print(f"Token è¾¹ç•Œæ•°: {len(token_boundaries)}")
        
        # æ˜¾ç¤ºè§„åˆ™ç±»å‹ç»Ÿè®¡
        rule_types = {}
        for rule in rules:
            rule_type = rule['type']
            rule_types[rule_type] = rule_types.get(rule_type, 0) + 1
        
        print(f"\nè¯­æ³•è§„åˆ™ç±»å‹ç»Ÿè®¡ (å‰10ç§):")
        sorted_rules = sorted(rule_types.items(), key=lambda x: x[1], reverse=True)[:10]
        for i, (rule_type, count) in enumerate(sorted_rules, 1):
            print(f"  {i:2d}. {rule_type}: {count}")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹è§„åˆ™
        print(f"\nç¤ºä¾‹è¯­æ³•è§„åˆ™ (å‰5ä¸ª):")
        for i, rule in enumerate(rules[:5], 1):
            text_preview = rule['text'].replace('\n', '\\n')
            print(f"  {i}. {rule['type']}: '{text_preview}'")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_code_samples():
    """æµ‹è¯• code_samples ç›®å½•ä¸­çš„æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ä»£ç æ ·æœ¬ç›®å½•")
    print("=" * 60)
    
    code_samples_dir = Path('./code_samples')
    if not code_samples_dir.exists():
        print("âŒ code_samples ç›®å½•ä¸å­˜åœ¨")
        return False
    
    print("âœ“ code_samples ç›®å½•å­˜åœ¨")
    
    # æ£€æŸ¥å„è¯­è¨€çš„æ ·æœ¬æ–‡ä»¶ç›®å½•
    expected_dirs = {
        'python': 'python',
        'javascript': 'javascript', 
        'typescript': 'typescript',
        'java': 'java',
        'c': 'c',
        'cpp': 'cpp',
        'csharp': 'csharp',
        'go': 'go',
        'ruby': 'ruby',
        'rust': 'rust',
        'scala': 'scala'
    }
    
    found_files = 0
    total_files = 0
    
    for lang, dirname in expected_dirs.items():
        dir_path = code_samples_dir / dirname
        if dir_path.exists() and dir_path.is_dir():
            # ç»Ÿè®¡ç›®å½•ä¸­çš„æ–‡ä»¶
            files = list(dir_path.glob('*'))
            file_count = len([f for f in files if f.is_file()])
            if file_count > 0:
                print(f"âœ“ {dirname}/ ({file_count} ä¸ªæ–‡ä»¶)")
                found_files += 1
                total_files += file_count
            else:
                print(f"âš ï¸  {dirname}/ (ç›®å½•ä¸ºç©º)")
        else:
            print(f"âŒ {dirname}/ ç›®å½•ä¸å­˜åœ¨")
    
    print(f"\næ‰¾åˆ° {found_files}/{len(expected_dirs)} ä¸ªè¯­è¨€ç›®å½•ï¼Œå…± {total_files} ä¸ªæ ·æœ¬æ–‡ä»¶")
    return found_files > 0

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("Quick Analyzer ç®€åŒ–æµ‹è¯•")
    print("æµ‹è¯• Tree-sitter rule-level alignment score è®¡ç®—åŠŸèƒ½")
    
    # æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
    core_test_passed = test_quick_analyzer_functionality()
    
    # æµ‹è¯•ä»£ç æ ·æœ¬
    samples_test_passed = test_code_samples()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    if core_test_passed:
        print("âœ“ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥")
    
    if samples_test_passed:
        print("âœ“ ä»£ç æ ·æœ¬æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ ä»£ç æ ·æœ¬æµ‹è¯•å¤±è´¥")
    
    if core_test_passed and samples_test_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥ä½¿ç”¨ quick_analyzer.py è¿›è¡Œå®Œæ•´åˆ†æ")
        print("\nå»ºè®®è¿è¡Œå‘½ä»¤:")
        print("  python quick_analyzer.py")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        if not core_test_passed:
            print("  - è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
            print("  - è¯·å…ˆè¿è¡Œ quick_analyzer.py æ¥ç¼–è¯‘è¯­è¨€åº“")
    
    return core_test_passed and samples_test_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)